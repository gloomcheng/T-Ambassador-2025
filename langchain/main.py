# -*- coding: utf-8 -*-
"""
æ™ºæ…§è²¡å‹™é¡§å•ç³»çµ±

é€™å€‹ç³»çµ±å±•ç¤ºäº†å¦‚ä½•å‰µå»ºä¸€å€‹æ™ºæ…§çš„è²¡å‹™é¡§å•ï¼Œèƒ½å¤ ï¼š
1. ç†è§£ç”¨æˆ¶æ„åœ–ä¸¦æ±ºå®šæ˜¯å¦éœ€è¦æœå°‹è³‡è¨Š
2. æ•´åˆçŸ¥è­˜åº«å’Œç¶²è·¯æœå°‹çµæœ
3. æä¾›å°ˆæ¥­çš„è²¡å‹™åˆ†æå»ºè­°

é€™æ˜¯ä¸€å€‹å°ˆæ¥­çš„è²¡å‹™é¡§å•ç³»çµ±ï¼
"""

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_ollama import ChatOllama
from langchain_community.embeddings.ollama import OllamaEmbeddings

# ===== è²¡å‹™å·¥å…·å‡½æ•¸ =====


def web_search(query: str) -> str:
    """ä½¿ç”¨ç¶²è·¯æœå°‹ç²å–æœ€æ–°è³‡è¨Š"""
    try:
        from ddgs import DDGS

        with DDGS() as ddgs:
            results = list(ddgs.text(query, max_results=2))

        if results:
            search_results = []
            for i, result in enumerate(results, 1):
                title = result.get('title', 'ç„¡æ¨™é¡Œ')
                body = result.get('body', 'ç„¡å…§å®¹')[:200]  # é™åˆ¶é•·åº¦
                search_results.append(f"{i}. {title}: {body}")

            result_text = "\n".join(search_results)
            return f"ğŸ” ç¶²è·¯æœå°‹çµæœï¼š\n{result_text}"
        else:
            return f"æŠ±æ­‰ï¼Œæ²’æœ‰æ‰¾åˆ°é—œæ–¼ã€Œ{query}ã€çš„æœå°‹çµæœã€‚"

    except Exception as e:
        return f"ç¶²è·¯æœå°‹å¤±æ•—ï¼š{str(e)}ã€‚è«‹ç¨å¾Œå†è©¦ã€‚"


def financial_analysis(company: str) -> str:
    """æä¾›è²¡å‹™åˆ†æå»ºè­°"""
    analysis_data = {
        "è˜‹æœå…¬å¸": "è˜‹æœå…¬å¸è²¡å‹™ç‹€æ³ç©©å¥ï¼Œç¾é‡‘æµå……æ²›ï¼Œè‚¡æ¯æ”¶ç›Šç‡ç´„ 0.6%ã€‚å»ºè­°é•·æœŸæŒæœ‰ã€‚",
        "å°ç©é›»": "å°ç©é›»æŠ€è¡“é ˜å…ˆï¼Œå®¢æˆ¶åŸºç¤å¼·å¤§ï¼Œé ä¼°æœªä¾†æˆé•·æ€§ä½³ã€‚ç•¶å‰æœ¬ç›Šæ¯”ç´„ 18 å€ï¼Œå±¬åˆç†æ°´æº–ã€‚",
        "ç‰¹æ–¯æ‹‰": "ç‰¹æ–¯æ‹‰æˆé•·è¿…é€Ÿä½†æ³¢å‹•å¤§ï¼Œé›»å‹•è»Šå¸‚å ´å‰æ™¯çœ‹å¥½ã€‚éœ€æ³¨æ„ç«¶çˆ­åŠ åŠ‡é¢¨éšªã€‚",
        "å¾®è»Ÿ": "å¾®è»Ÿé›²ç«¯æ¥­å‹™ç©©å¥æˆé•·ï¼ŒAI æŠ•è³‡å¯æœ›å¸¶ä¾†é•·æœŸæ”¶ç›Šã€‚è²¡å‹™ç‹€æ³æ¥µä½³ã€‚",
        "è°·æ­Œ": "è°·æ­Œå»£å‘Šæ¥­å‹™ç©©å®šï¼Œé›²ç«¯é‹ç®—æŒçºŒæ“´å¼µï¼Œè²¡å‹™ç‹€æ³è‰¯å¥½ã€‚",
        "äºé¦¬éœ": "äºé¦¬éœé›»å•†éœ¸ä¸»åœ°ä½ç©©å›ºï¼Œé›²ç«¯æœå‹™æˆé•·å¼·å‹ï¼Œé•·æœŸæŠ•è³‡åƒ¹å€¼é«˜ã€‚"
    }

    supported_companies = "è˜‹æœå…¬å¸ã€å°ç©é›»ã€ç‰¹æ–¯æ‹‰ã€å¾®è»Ÿã€è°·æ­Œã€äºé¦¬éœ"
    return analysis_data.get(company, f"æŠ±æ­‰ï¼Œæˆ‘æ²’æœ‰ {company} çš„è²¡å‹™åˆ†æè³‡æ–™ã€‚"
                                     f"æ”¯æ´çš„å…¬å¸ï¼š{supported_companies}")


def detect_intent(question: str) -> tuple[str, str]:
    """åµæ¸¬ç”¨æˆ¶æ„åœ–ä¸¦æ±ºå®šéœ€è¦çš„è³‡è¨Šé¡å‹"""
    question_lower = question.lower()

    # è‚¡åƒ¹æŸ¥è©¢
    stock_keywords = ['è‚¡åƒ¹', 'è‚¡ç¥¨åƒ¹æ ¼', 'å¸‚å€¼', 'æˆäº¤é‡']
    stock_companies = ['è˜‹æœ', 'å°ç©é›»', 'ç‰¹æ–¯æ‹‰', 'å¾®è»Ÿ', 'è°·æ­Œ', 'äºé¦¬éœ']

    for company in stock_companies:
        if company in question:
            return "stock", company

    if any(keyword in question_lower for keyword in stock_keywords):
        return "stock", "general"

    # æ–°èæŸ¥è©¢
    news_keywords = ['æ–°è', 'æœ€æ–°æ¶ˆæ¯', 'å¸‚å ´å‹•æ…‹', 'è²¡ç¶“æ–°è']
    if any(keyword in question_lower for keyword in news_keywords):
        return "news", question

    # è²¡å‹™åˆ†æ
    analysis_keywords = ['åˆ†æ', 'æŠ•è³‡åƒ¹å€¼', 'å»ºè­°', 'è©•ä¼°']
    for company in stock_companies:
        if (company in question and
            any(keyword in question_lower for keyword in analysis_keywords)):
            return "analysis", company

    # é è¨­ç‚ºçŸ¥è­˜åº«æŸ¥è©¢
    return "knowledge", question


# ===== è²¡å‹™é¡§å•ç³»çµ± =====

def create_financial_advisor():
    """å‰µå»ºè²¡å‹™é¡§å•ç³»çµ±"""

    # åˆå§‹åŒ–è²¡å‹™çŸ¥è­˜åº«
    try:
        loader = PyPDFLoader("202502_6625_AI1_20250924_142829.pdf")
        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        doc_split = loader.load_and_split(text_splitter=splitter)

        embeddings = OllamaEmbeddings(model="nomic-embed-text")
        vectorstore = Chroma.from_documents(documents=doc_split, embedding=embeddings)
        retriever = vectorstore.as_retriever()
        print("âœ… çŸ¥è­˜åº«è¼‰å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âš ï¸ çŸ¥è­˜åº«è¼‰å…¥å¤±æ•—ï¼Œä½¿ç”¨é è¨­çŸ¥è­˜ï¼š{str(e)}")
        # å‰µå»ºä¸€å€‹ç©ºçš„æª¢ç´¢å™¨ï¼Œå¦‚æœæ²’æœ‰ PDF æª”æ¡ˆ
        from langchain_core.retrievers import BaseRetriever
        from langchain_core.documents import Document

        class EmptyRetriever(BaseRetriever):
            def get_relevant_documents(self, query):
                content = ("é€™æ˜¯ä¸€å€‹è²¡å‹™çŸ¥è­˜åº«çš„é è¨­å›æ‡‰ã€‚"
                          "ç”±æ–¼æ²’æœ‰è¼‰å…¥ PDF æ–‡ä»¶ï¼Œé€™è£¡æä¾›åŸºæœ¬çš„è²¡å‹™çŸ¥è­˜ã€‚")
                return [Document(page_content=content)]

        retriever = EmptyRetriever()

    # åˆå§‹åŒ– LLM
    llm = ChatOllama(model="gemma3:1b", temperature=0.1)

    return llm, retriever

def answer_financial_question(question: str, llm, retriever):
    """æ™ºæ…§å›ç­”è²¡å‹™å•é¡Œ"""

    # åµæ¸¬ç”¨æˆ¶æ„åœ–
    intent, target = detect_intent(question)

    print(f"ğŸ¤” åµæ¸¬æ„åœ–ï¼š{intent} - {target}")

    if intent == "stock":
        # è‚¡åƒ¹æŸ¥è©¢
        if target == "general":
            search_query = question  # ç”¨æˆ¶å•é¡Œä½œç‚ºæœå°‹æŸ¥è©¢
        else:
            search_query = f"{target} è‚¡åƒ¹"

        print(f"ğŸ“ˆ æœå°‹è‚¡åƒ¹è³‡è¨Šï¼š{search_query}")
        search_result = web_search(search_query)

        # æ•´åˆæœå°‹çµæœå’ŒçŸ¥è­˜åº«è³‡è¨Š
        relevant_docs = retriever.get_relevant_documents(question)
        context = relevant_docs[0].page_content if relevant_docs else ""

        # å‰µå»ºæ•´åˆæç¤º
        context_text = context[:1000] if context else ""
        combined_prompt = (
            "æ ¹æ“šä»¥ä¸‹è³‡è¨Šå›ç­”ç”¨æˆ¶å•é¡Œï¼š"
            f"çŸ¥è­˜åº«å…§å®¹ï¼š{context_text}"
            f"ç¶²è·¯æœå°‹çµæœï¼š{search_result}"
            f"ç”¨æˆ¶å•é¡Œï¼š{question}"
            "è«‹æä¾›å°ˆæ¥­ã€æº–ç¢ºçš„è²¡å‹™å›ç­”ã€‚"
        )

        response = llm.invoke(combined_prompt)
        return response.content

    elif intent == "news":
        # æ–°èæŸ¥è©¢
        print(f"ğŸ“° æœå°‹æ–°èè³‡è¨Šï¼š{target}")
        search_result = web_search(target)

        # æ•´åˆæ–°èå’ŒçŸ¥è­˜åº«è³‡è¨Š
        relevant_docs = retriever.get_relevant_documents(question)
        context = relevant_docs[0].page_content if relevant_docs else ""

        context_text = context[:1000] if context else ""
        combined_prompt = (
            "æ ¹æ“šä»¥ä¸‹è³‡è¨Šå›ç­”ç”¨æˆ¶å•é¡Œï¼š"
            f"çŸ¥è­˜åº«å…§å®¹ï¼š{context_text}"
            f"æœ€æ–°æ–°èè³‡è¨Šï¼š{search_result}"
            f"ç”¨æˆ¶å•é¡Œï¼š{question}"
            "è«‹æä¾›å°ˆæ¥­ã€å®¢è§€çš„æ–°èåˆ†æã€‚"
        )

        response = llm.invoke(combined_prompt)
        return response.content

    elif intent == "analysis":
        # è²¡å‹™åˆ†æ
        print(f"ğŸ’¼ æä¾›è²¡å‹™åˆ†æï¼š{target}")
        analysis_result = financial_analysis(target)

        # ä¹Ÿå¯ä»¥æ•´åˆæœå°‹çµæœä¾†å¢å¼·åˆ†æ
        search_query = f"{target} æœ€æ–°è²¡å‹™ç‹€æ³"
        search_result = web_search(search_query)

        combined_prompt = (
            "æ ¹æ“šä»¥ä¸‹è³‡è¨Šæä¾›è²¡å‹™åˆ†æï¼š"
            f"è²¡å‹™åˆ†æè³‡æ–™ï¼š{analysis_result}"
            f"æœ€æ–°å¸‚å ´è³‡è¨Šï¼š{search_result}"
            f"ç”¨æˆ¶å•é¡Œï¼š{question}"
            "è«‹æä¾›å°ˆæ¥­ã€å…¨é¢çš„è²¡å‹™åˆ†æå»ºè­°ã€‚"
        )

        response = llm.invoke(combined_prompt)
        return response.content

    else:
        # çŸ¥è­˜åº«æŸ¥è©¢
        print("ğŸ“š æŸ¥è©¢çŸ¥è­˜åº«è³‡è¨Š")
        relevant_docs = retriever.get_relevant_documents(question)
        context = relevant_docs[0].page_content if relevant_docs else "æ²’æœ‰æ‰¾åˆ°ç›¸é—œè³‡è¨Š"

        context_text = context[:2000] if context else ""
        prompt = (
            "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„è²¡å‹™é¡§å•ã€‚è«‹æ ¹æ“šä»¥ä¸‹çŸ¥è­˜åº«å…§å®¹å›ç­”å•é¡Œï¼š"
            f"çŸ¥è­˜åº«å…§å®¹ï¼š{context_text}"
            f"ç”¨æˆ¶å•é¡Œï¼š{question}"
            "è«‹æä¾›å°ˆæ¥­ã€æº–ç¢ºçš„å›ç­”ã€‚"
        )

        response = llm.invoke(prompt)
        return response.content


# ===== ä¸»ç¨‹å¼ =====

def main():
    print("ğŸ¤– æ™ºæ…§è²¡å‹™é¡§å•ç³»çµ±")
    print("=" * 60)
    print("ğŸ’¼ é€™å€‹ç³»çµ±èƒ½å¤ æ™ºæ…§æ±ºå®šä½•æ™‚æœå°‹è³‡è¨Šä¸¦æä¾›å°ˆæ¥­å»ºè­°")

    # åˆå§‹åŒ–ç³»çµ±
    llm, retriever = create_financial_advisor()

    print("\nâœ… è²¡å‹™é¡§å•ç³»çµ±æº–å‚™å®Œæˆï¼")
    print("=" * 60)
    print("ğŸ’¡ ç³»çµ±ç‰¹è‰²ï¼š")
    print("â€¢ è‡ªå‹•åµæ¸¬ç”¨æˆ¶æ„åœ–ï¼ˆè‚¡åƒ¹ã€æ–°èã€åˆ†æã€çŸ¥è­˜æŸ¥è©¢ï¼‰")
    print("â€¢ æ™ºæ…§æ•´åˆçŸ¥è­˜åº«å’Œç¶²è·¯æœå°‹çµæœ")
    print("â€¢ æä¾›å°ˆæ¥­çš„è²¡å‹™åˆ†æå»ºè­°")
    print("â€¢ æ”¯æ´å³æ™‚è‚¡åƒ¹å’Œæ–°èæŸ¥è©¢")

    print("\nğŸ¯ è©¦è©¦é€™äº›å•é¡Œï¼š")
    print("â€¢ ã€Œè˜‹æœè‚¡åƒ¹å¤šå°‘ï¼Ÿã€ï¼ˆè‡ªå‹•æœå°‹æœ€æ–°è‚¡åƒ¹ï¼‰")
    print("â€¢ ã€Œåˆ†æè˜‹æœå…¬å¸çš„æŠ•è³‡åƒ¹å€¼ã€ï¼ˆæ•´åˆåˆ†æï¼‰")
    print("â€¢ ã€Œæœ€è¿‘ç§‘æŠ€è‚¡æœ‰ä»€éº¼æ–°èï¼Ÿã€ï¼ˆæœå°‹æœ€æ–°æ–°èï¼‰")
    print("â€¢ ã€Œæˆ‘æƒ³æŠ•è³‡é›»å‹•è»Šç”¢æ¥­ï¼Œä½ æœ‰ä»€éº¼å»ºè­°ï¼Ÿã€ï¼ˆç¶œåˆå»ºè­°ï¼‰")
    print("â€¢ ã€Œå°ç©é›»è‚¡åƒ¹æ€éº¼æ¨£ï¼Ÿã€ï¼ˆå°è‚¡æŸ¥è©¢ï¼‰")
    print("â€¢ ã€Œå¿…æ‡‰æ•´é«”æ˜¯è³ºéŒ¢çš„å—ï¼Ÿã€ï¼ˆçŸ¥è­˜åº«æŸ¥è©¢ï¼‰")

    while True:
        try:
            question = input("\nâ“ è«‹è¼¸å…¥è²¡å‹™å•é¡Œï¼ˆè¼¸å…¥ 'exit' çµæŸï¼‰ï¼š")

            if question.lower() == 'exit':
                print("ğŸ‘‹ æ„Ÿè¬ä½¿ç”¨è²¡å‹™é¡§å•ï¼Œå†è¦‹ï¼")
                break

            if question.strip():
                print(f"\nğŸ” åˆ†æå•é¡Œï¼š{question}")
                print("-" * 50)

                # æ™ºæ…§å›ç­”å•é¡Œ
                answer = answer_financial_question(question, llm, retriever)

                print(f"\nğŸ’¼ è²¡å‹™é¡§å•å›ç­”ï¼š{answer}")

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç”¨æˆ¶ä¸­æ–·ï¼Œæ„Ÿè¬ä½¿ç”¨è²¡å‹™é¡§å•ï¼")
            break
        except Exception as e:
            print(f"âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")


if __name__ == "__main__":
    main()
