# -*- coding: utf-8 -*-
"""
ç‰ˆæœ¬ 1ï¼šåŸºæœ¬çš„è²¡å‹™çŸ¥è­˜åº« QA ç³»çµ±

é€™æ˜¯æœ€ç°¡å–®çš„å•ç­”ç³»çµ±ï¼Œåªä½¿ç”¨çŸ¥è­˜åº«ä¾†å›ç­”è²¡å‹™ç›¸é—œå•é¡Œã€‚
æ²’æœ‰å·¥å…·ï¼Œä¹Ÿæ²’æœ‰ Agent é‚è¼¯ã€‚
"""

# å°å…¥å¿…è¦çš„æ¨¡çµ„
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.chat_models import ChatOllama
from langchain_community.embeddings.ollama import OllamaEmbeddings
from langchain_core.prompts import ChatPromptTemplate

def main():
    print("ğŸ¤– ç‰ˆæœ¬ 1ï¼šåŸºæœ¬çš„è²¡å‹™çŸ¥è­˜åº« QA ç³»çµ±")
    print("=" * 60)
    print("ğŸ“Š é€™å€‹ç³»çµ±åªèƒ½å›ç­”çŸ¥è­˜åº«ä¸­çš„è²¡å‹™å•é¡Œ")

    # è¼‰å…¥ PDF æ–‡ä»¶
    print("ğŸ“š è¼‰å…¥è²¡å‹™çŸ¥è­˜åº«...")
    loader = PyPDFLoader("202502_6625_AI1_20250924_142829.pdf")
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    doc_split = loader.load_and_split(text_splitter=splitter)

    # å»ºç«‹å‘é‡è³‡æ–™åº«
    print("ğŸ” å»ºç«‹è²¡å‹™çŸ¥è­˜å‘é‡è³‡æ–™åº«...")
    embeddings = OllamaEmbeddings(model="nomic-embed-text")
    vectorstore = Chroma.from_documents(documents=doc_split, embedding=embeddings)
    retriever = vectorstore.as_retriever()

    # åˆå§‹åŒ– LLM
    print("ğŸ§  åˆå§‹åŒ–è²¡å‹™åˆ†ææ¨¡å‹...")
    llm = ChatOllama(model="gemma3:1b", temperature=0.1)

    # å»ºç«‹è²¡å‹™åˆ†æå°è©±æ¨¡æ¿
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„è²¡ç¶“åˆ†æåŠ©æ‰‹ã€‚è«‹æ ¹æ“šæä¾›çš„è²¡å‹™æ–‡ä»¶å…§å®¹å›ç­”å•é¡Œã€‚åªå›ç­”çŸ¥è­˜åº«ä¸­æœ‰çš„è³‡è¨Šã€‚"),
        ("user", "è²¡å‹™æ–‡ä»¶å…§å®¹ï¼š{context}\n\nå•é¡Œï¼š{question}")
    ])

    print("\nâœ… ç³»çµ±æº–å‚™å®Œæˆï¼ç¾åœ¨å¯ä»¥é–‹å§‹å•è²¡å‹™å•é¡Œäº†ã€‚")
    print("=" * 60)
    print("ğŸ’¡ é€™å€‹ç‰ˆæœ¬åªèƒ½å›ç­”çŸ¥è­˜åº«ä¸­çš„å•é¡Œï¼Œç„¡æ³•æŸ¥è©¢å³æ™‚è³‡è¨Šã€‚")
    print()
    print("ç¯„ä¾‹å•é¡Œï¼š")
    print("â€¢ ã€Œå¿…æ‡‰æ•´é«”æ˜¯è³ºéŒ¢çš„å—ï¼Ÿã€")
    print("â€¢ ã€Œé€™å®¶å…¬å¸ä¸»è¦çš„æ¥­å‹™æ˜¯ä»€éº¼ï¼Ÿã€")
    print("â€¢ ã€Œå…¬å¸çš„è²¡å‹™ç‹€æ³å¦‚ä½•ï¼Ÿã€")

    while True:
        try:
            question = input("\nâ“ è«‹è¼¸å…¥è²¡å‹™å•é¡Œï¼ˆè¼¸å…¥ 'exit' çµæŸï¼‰ï¼š")

            if question.lower() == 'exit':
                print("ğŸ‘‹ å†è¦‹ï¼")
                break

            if question.strip():
                print(f"\nğŸ” æœå°‹è²¡å‹™çŸ¥è­˜åº«...")
                # å–å¾—ç›¸é—œæ–‡ä»¶
                relevant_docs = retriever.get_relevant_documents(question)
                context = relevant_docs[0].page_content if relevant_docs else "æ²’æœ‰æ‰¾åˆ°ç›¸é—œè²¡å‹™è³‡è¨Š"

                # ç”¢ç”Ÿè²¡å‹™åˆ†æå›ç­”
                response = llm.invoke(prompt.format_messages(
                    context=context[:2000],  # é™åˆ¶ä¸Šä¸‹æ–‡é•·åº¦
                    question=question
                ))

                print(f"\nğŸ¤– è²¡å‹™åˆ†æå›ç­”ï¼š{response.content}")

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç”¨æˆ¶ä¸­æ–·ï¼Œå†è¦‹ï¼")
            break
        except Exception as e:
            print(f"âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")

if __name__ == "__main__":
    main()
